{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install --force-reinstall openai==1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from settings import *\n",
    "from utils import preprocess, prompt, score, utils\n",
    "import os \n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "from langchain.prompts import load_prompt, PromptTemplate\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "                                                             \n",
    "config = utils.load_json(CONFIG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a review of the literature and an extensive me...</td>\n",
       "      <td>backgrounda review of the literature and an ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nathan , as an oncology fellow , knew well tha...</td>\n",
       "      <td>t cells tell macrophages when to start making ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>temporary henna tattoos or pseudotattoo have b...</td>\n",
       "      <td>temporary henna tattoos or pseudotattoos have ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article   \n",
       "0  a review of the literature and an extensive me...  \\\n",
       "1  nathan , as an oncology fellow , knew well tha...   \n",
       "2  temporary henna tattoos or pseudotattoo have b...   \n",
       "\n",
       "                                            abstract  \n",
       "0  backgrounda review of the literature and an ex...  \n",
       "1  t cells tell macrophages when to start making ...  \n",
       "2  temporary henna tattoos or pseudotattoos have ...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubmed = utils.load_data(config['data_name'])\n",
    "pubmed.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d826407cc684872a9120df1e8724990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "cache_dir = \"/data/ephemeral/Youtube-Short-Generator/mistral\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,cache_dir=cache_dir)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300, device = 0, pad_token_id=tokenizer.eos_token_id)\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<s>[INST]<>You are an abstractive summarizer that follows the output pattern.\n",
    "Please revise the extracted summary based on the document. The revised summary should include the information in the extracted summary. Original Document: {document}<>[/INST]<\\s>.\n",
    "\"\"\"\n",
    "doc = pubmed.iloc[0, 0]\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"document\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=hf)\n",
    "response = llm_chain.invoke(input = doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarization(df,save_name, iter_num = 5):\n",
    "    template = \"\"\"\n",
    "    <s>[INST]<>You are an abstractive summarizer that follows the output pattern.\n",
    "    Please revise the extracted summary based on the document. The revised summary should include the information in the extracted summary. Original Document: {document}<>[/INST]<\\s>.\n",
    "    \"\"\"\n",
    "    for i in range(iter_num):\n",
    "        response_list = []\n",
    "        for idx in tqdm(range(len(df))):\n",
    "            doc = df.iloc[idx, 0]\n",
    "            prompt = PromptTemplate(template=template, input_variables=[\"document\"])\n",
    "            llm_chain = LLMChain(prompt=prompt, llm=hf)\n",
    "            response = llm_chain.invoke(input = doc)['text']\n",
    "            if len(response) > 0:\n",
    "                response_list.append([response, df.iloc[idx, 1]])\n",
    "        df = pd.DataFrame(response_list, columns = ['generate', 'abstract'])\n",
    "        df.to_csv(os.path.join(OUT_DIR, f\"{save_name}_{i}.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rouge_from_df(generate_df, rouge_type = 'rouge-l', metric = 'f'):\n",
    "    df = pd.read_csv(os.path.join(OUT_DIR, generate_df))   \n",
    "    value = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        value_dic = score.get_Rouge_score(row['generate'], row['abstract'])\n",
    "        value += value_dic[rouge_type][metric]\n",
    "    return value / len(generate_df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rouge_from_all_df(save_name):\n",
    "    file_list = [i for i in os.listdir(OUT_DIR) if save_name in i]\n",
    "    value_list = []\n",
    "    for file in file_list:\n",
    "        value = get_rouge_from_df(file)\n",
    "        value_list.append(value)\n",
    "    return sum(value_list) / len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:19<00:00, 13.90s/it]\n"
     ]
    }
   ],
   "source": [
    "test = pubmed.iloc[:10]\n",
    "save_name = 'test'\n",
    "get_summarization(test, save_name, 1)\n",
    "model_avg_rouge = get_rouge_from_all_df(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3529029323833017"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_avg_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
